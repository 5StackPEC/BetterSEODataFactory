{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List\n",
    "\n",
    "# types\n",
    "import concurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lighthouse(url):\n",
    "    cmd = f\"lighthouse {url} --output=json --quiet\"\n",
    "    result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error running Lighthouse command for {url}: {result.stderr.decode()}\")\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            return json.loads(result.stdout)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON for {url}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(url):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    driver.get(url)\n",
    "    report = run_lighthouse(url)\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_report(report, filename, url, report_id):\n",
    "    if report is not None:\n",
    "        important_scores = {\n",
    "            'id': report_id,\n",
    "            'url': url,\n",
    "            'performance': report['categories']['performance']['score'] * 100,\n",
    "            'accessibility': report['categories']['accessibility']['score'] * 100,\n",
    "            'best-practices': report['categories']['best-practices']['score'] * 100,\n",
    "            'seo': report['categories']['seo']['score'] * 100\n",
    "        }\n",
    "        \n",
    "        # Read the existing content if the file exists\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r') as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                except json.JSONDecodeError:\n",
    "                    data = {'lighthouseReports': []}\n",
    "        else:\n",
    "            data = {'lighthouseReports': []}\n",
    "        \n",
    "        # Append the new report to the list\n",
    "        data['lighthouseReports'].append(important_scores)\n",
    "        \n",
    "        # Write the updated data back to the file\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "            print(f\"Report saved for {filename}\")\n",
    "        \n",
    "        # Print scores to the console\n",
    "        print(\"Lighthouse Performance Metrics:\")\n",
    "        for category, score in important_scores.items():\n",
    "            if category not in ['id', 'url']:\n",
    "                print(f\"{category.capitalize()} score: {score}\")\n",
    "    else:\n",
    "        print(f\"No report generated for {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_report(csvreader):\n",
    "    for report_id, row in enumerate(csvreader):\n",
    "            url = row[0]\n",
    "            print(url)\n",
    "            report = generate_report(url)\n",
    "            # filename = f\"lighthouse_report_{url.replace('https://', '').replace('/', '_')}.json\"\n",
    "            save_report(report, \"lighthouse_report.json\", url, report_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://1k62.com \n",
      "\n",
      "Something went wrong while generating the report\n",
      "Message: unknown error: net::ERR_NAME_NOT_RESOLVED\n",
      "  (Session info: chrome-headless-shell=125.0.6422.77)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7BFE31F22+60322]\n",
      "\t(No symbol) [0x00007FF7BFDACE99]\n",
      "\t(No symbol) [0x00007FF7BFC67EBA]\n",
      "\t(No symbol) [0x00007FF7BFC5FEA2]\n",
      "\t(No symbol) [0x00007FF7BFC509C4]\n",
      "\t(No symbol) [0x00007FF7BFC52212]\n",
      "\t(No symbol) [0x00007FF7BFC50CF1]\n",
      "\t(No symbol) [0x00007FF7BFC504D1]\n",
      "\t(No symbol) [0x00007FF7BFC50410]\n",
      "\t(No symbol) [0x00007FF7BFC4E39D]\n",
      "\t(No symbol) [0x00007FF7BFC4E9DC]\n",
      "\t(No symbol) [0x00007FF7BFC6AC91]\n",
      "\t(No symbol) [0x00007FF7BFCFC5FE]\n",
      "\t(No symbol) [0x00007FF7BFCDC21A]\n",
      "\t(No symbol) [0x00007FF7BFCFBC80]\n",
      "\t(No symbol) [0x00007FF7BFCDBFC3]\n",
      "\t(No symbol) [0x00007FF7BFCA9617]\n",
      "\t(No symbol) [0x00007FF7BFCAA211]\n",
      "\tGetHandleVerifier [0x00007FF7C014946D+3301613]\n",
      "\tGetHandleVerifier [0x00007FF7C0193693+3605267]\n",
      "\tGetHandleVerifier [0x00007FF7C0189410+3563664]\n",
      "\tGetHandleVerifier [0x00007FF7BFEE42F6+790390]\n",
      "\t(No symbol) [0x00007FF7BFDB74DF]\n",
      "\t(No symbol) [0x00007FF7BFDB33D4]\n",
      "\t(No symbol) [0x00007FF7BFDB3562]\n",
      "\t(No symbol) [0x00007FF7BFDA2F6F]\n",
      "\tBaseThreadInitThunk [0x00007FFC12207344+20]\n",
      "\tRtlUserThreadStart [0x00007FFC13F226B1+33]\n",
      "\n",
      "http://abe-kamery.pl \n",
      "\n",
      "Report saved for lighthouse_report.json\n",
      "Lighthouse Performance Metrics:\n",
      "Performance score: 91.0\n",
      "Accessibility score: 63.0\n",
      "Best-practices score: 76.0\n",
      "Seo score: 83.0\n",
      "http://acvrn.ru \n",
      "\n",
      "Report saved for lighthouse_report.json\n",
      "Lighthouse Performance Metrics:\n",
      "Performance score: 21.0\n",
      "Accessibility score: 64.0\n",
      "Best-practices score: 34.0\n",
      "Seo score: 92.0\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "\n",
    "def process_chunk(chunk, start_id):\n",
    "    for report_id, row in enumerate(chunk):\n",
    "        url = row[0]\n",
    "        print(url, \"\\n\")\n",
    "        try:\n",
    "            report = generate_report(url)\n",
    "            save_report(report, \"lighthouse_report.json\", url, start_id + report_id)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                \"Something went wrong while generating the report\"\n",
    "            )  # Delete the try/except to see more details or print an \"e\"\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "\n",
    "def chunked_csv_reader(file_path, chunk_size):\n",
    "    with open(file_path, newline=\"\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        next(csvreader)  # Skip the header\n",
    "        chunk = []\n",
    "        for row in csvreader:\n",
    "            chunk.append(row)\n",
    "            if len(chunk) == chunk_size:\n",
    "                yield chunk\n",
    "                chunk = []\n",
    "        if chunk:  # Yield the last chunk if it's not empty\n",
    "            yield chunk\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chunk_size = 10  # Adjust chunk size as needed\n",
    "    file_path = \"./WebScreenshots.csv\"\n",
    "\n",
    "    with ThreadPoolExecutor(\n",
    "        max_workers=10\n",
    "    ) as executor:  # Adjust number of workers as needed\n",
    "        futures: List[concurrent.futures._base.Future] = []\n",
    "        start_id = 0\n",
    "        for chunk in chunked_csv_reader(file_path, chunk_size):\n",
    "            futures.append(executor.submit(process_chunk, chunk, start_id))\n",
    "            start_id += len(chunk)\n",
    "\n",
    "        for future in futures:\n",
    "            future.result()  # Wait for all futures to complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
